{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "- 사용 데이터: QMNIST\n",
    "- 데이터 크기: (60000, 28, 28) \n",
    "\n",
    "이 튜토리얼은 QMNIST 숫자를 분류하기 위해 다층 퍼셉트론(Multi-Layer Perceptron)을 훈련합니다. <br>\n",
    "이 모델을 QMNIST 테스트 세트에서 96% 정확도를 달성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 임포트하기\n",
    "- torch 라이브러리 install을 위해 참고할 사이트: https://pytorch.org/get-started/locally/\n",
    "- numpy 라이브러리 install을 위해 사용한 코드: `pip install numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드\n",
    "- 데이터 불러오기(pytorch에 내장된 데이터, 처음 실행 시 지정 폴더에 다운됨)\n",
    "- 데이터 구성을, 학습/테스트 형태로 구분\n",
    "- MNIST 데이터와 같이 비전 모델 개발에 자주 사용되는 데이터는 학습, 테스트 데이터 구성 이미 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/facebookresearch/qmnist/master/qmnist-test-images-idx3-ubyte.gz to /data\\QMNIST\\raw\\qmnist-test-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9742279/9742279 [01:09<00:00, 140523.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data\\QMNIST\\raw\\qmnist-test-images-idx3-ubyte.gz to /data\\QMNIST\\raw\n",
      "Downloading https://raw.githubusercontent.com/facebookresearch/qmnist/master/qmnist-test-labels-idx2-int.gz to /data\\QMNIST\\raw\\qmnist-test-labels-idx2-int.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526800/526800 [00:00<00:00, 10160294.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data\\QMNIST\\raw\\qmnist-test-labels-idx2-int.gz to /data\\QMNIST\\raw"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize() : 이미지 사이즈 변경\n",
    "    transforms.ToTensor(),                          # 이미지를 텐서로 변환(다차원 배열구조)\n",
    "    transforms.Normalize((0.1307,), (0.3081))       # 데이터 평균, 표준편차 값 활용해 정규화(ex. 0~255 -> -1~1)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.QMNIST('/data', train=True,  transform=transform, download=True)    # 지정 폴더 = 코드 실행 위치 내 data라는 이름의 폴더\n",
    "test_dataset = datasets.QMNIST('/data', train=False, transform=transform, download=True)                    # 학습, 테스트 데이터 구분해 다운\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)       # batch_size: 모델에 한번에 입력할 sample 수\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)      # shuffle: 데이터 랜덤하"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 설계\n",
    "위 모델은 입력층, 은닉층, 출력층으로 이루어져 있으며, ReLU 활성화 함수를 사용한 세 개의 완전 연결 층으로 구성되어 있습니다. \n",
    "\n",
    "### MLP 구현\n",
    "- QMNIST dataset(28*28, 60000) -> Input layer(784=28x28) -> Hidden layer(512) -> output layer(256) -> Hidden layer(256) -> output layer(10)\n",
    "- 출력 값은 0~9(10가지) 중 하나에 해당\n",
    "- torch.nn.Module을 상속받아 모델 작성\n",
    "\n",
    "### 모델 객체 생성\n",
    "- 손실함수(criterion): Negative Log Likelihood Loss\n",
    "- 최적화: 확률적 경사 하강법 사용\n",
    "- lr: learning rate 크기가 클수로 빠른 속도 학습, 정답 착기 어려움(작으면 속도 느림)\n",
    "- momentum: 로컬 미니마에 빠질 위험을 줄여주기 우한 요소(0~1, 값이 클수록 영향력 큼)\n",
    "\n",
    "### 모델 학습 메소드 구현\n",
    "- device: GPU 사용 여부\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "model = MLP()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):   \n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   # 전체 데이터를 barch size만큼 읽어오는 과정 반복\n",
    "        data, target = data.to(device), target.to(device)       # GPU 사용시 CPU -> GPU\n",
    "        optimizer.zero_grad()       # Gradient 초기화, 매 학습시마다 초기화 후 backward\n",
    "        output = model(data)        # 데이터를 MLP에 통과치켜 output 획득\n",
    "        loss = criterion(output, target)        # output 벡터와 정답을 이용해 손실값 계산\n",
    "        loss.backward()             # 계산된 손실값으로 backward 진행(변화량 계산, 미분 계산)\n",
    "        optimizer.step()            # 파라미터 업데이트\n",
    "        if batch_idx % 100 == 0:    # 결과값 10번 반복마다 print\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}({100.*batch_idx/len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, epoch):\n",
    "    model.eval()        # 모델 평가 시, eval() 적용해 평가모드 진입(학습시 필요 연산 비활성화)\n",
    "    test_loss = 0       # 테스트 데이터에 대한 누적 손실값 계산을 위한 변수\n",
    "    correct = 0         # 테스트 데이터들 중, 정답을 맞춘 데이터의 수를 세기 위한 변수\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()       # 손실함수를 통한 손실값 계산\n",
    "            pred = output.argmax(dim=1, keepdim=True)           # 0~9중 가장 높은 확률에 해당하는 값\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()   # 예측치와 실제 닶이 같은지 비교\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)           # 전체 데이터 수로 누적값을 나눠, 평균 손실값 계산\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy:{correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n', end='\\r')\n",
    "    # 전체 데이터에 대해 정답을 맞춘 비율(정확도) 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장 및 불러오기 함수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch):\n",
    "    torch.save(model.state_dict(), 'models/mnist_mlp_model{}.pth'.format(epoch))\n",
    "\n",
    "def load_model(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습과 저장\n",
    "다음 코드 결과 .pth로 저장 / models 파일 생성 해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000(0%)]\tLoss: 2.324884\n",
      "Train Epoch: 1 [6400/60000(11%)]\tLoss: 1.092968\n",
      "Train Epoch: 1 [12800/60000(21%)]\tLoss: 0.610828\n",
      "Train Epoch: 1 [19200/60000(32%)]\tLoss: 0.363984\n",
      "Train Epoch: 1 [25600/60000(43%)]\tLoss: 0.268792\n",
      "Train Epoch: 1 [32000/60000(53%)]\tLoss: 0.339016\n",
      "Train Epoch: 1 [38400/60000(64%)]\tLoss: 0.370975\n",
      "Train Epoch: 1 [44800/60000(75%)]\tLoss: 0.440110\n",
      "Train Epoch: 1 [51200/60000(85%)]\tLoss: 0.343822\n",
      "Train Epoch: 1 [57600/60000(96%)]\tLoss: 0.164796\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy:55214/60000 (92%)\n",
      "Train Epoch: 2 [0/60000(0%)]\tLoss: 0.283648\n",
      "Train Epoch: 2 [6400/60000(11%)]\tLoss: 0.287534\n",
      "Train Epoch: 2 [12800/60000(21%)]\tLoss: 0.358240\n",
      "Train Epoch: 2 [19200/60000(32%)]\tLoss: 0.373477\n",
      "Train Epoch: 2 [25600/60000(43%)]\tLoss: 0.156273\n",
      "Train Epoch: 2 [32000/60000(53%)]\tLoss: 0.239027\n",
      "Train Epoch: 2 [38400/60000(64%)]\tLoss: 0.176559\n",
      "Train Epoch: 2 [44800/60000(75%)]\tLoss: 0.140408\n",
      "Train Epoch: 2 [51200/60000(85%)]\tLoss: 0.310318\n",
      "Train Epoch: 2 [57600/60000(96%)]\tLoss: 0.178305\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy:56419/60000 (94%)\n",
      "Train Epoch: 3 [0/60000(0%)]\tLoss: 0.223789\n",
      "Train Epoch: 3 [6400/60000(11%)]\tLoss: 0.179989\n",
      "Train Epoch: 3 [12800/60000(21%)]\tLoss: 0.135555\n",
      "Train Epoch: 3 [19200/60000(32%)]\tLoss: 0.148474\n",
      "Train Epoch: 3 [25600/60000(43%)]\tLoss: 0.076941\n",
      "Train Epoch: 3 [32000/60000(53%)]\tLoss: 0.090039\n",
      "Train Epoch: 3 [38400/60000(64%)]\tLoss: 0.132493\n",
      "Train Epoch: 3 [44800/60000(75%)]\tLoss: 0.260573\n",
      "Train Epoch: 3 [51200/60000(85%)]\tLoss: 0.299696\n",
      "Train Epoch: 3 [57600/60000(96%)]\tLoss: 0.108830\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy:57197/60000 (95%)\n",
      "Train Epoch: 4 [0/60000(0%)]\tLoss: 0.093785\n",
      "Train Epoch: 4 [6400/60000(11%)]\tLoss: 0.122626\n",
      "Train Epoch: 4 [12800/60000(21%)]\tLoss: 0.168886\n",
      "Train Epoch: 4 [19200/60000(32%)]\tLoss: 0.130343\n",
      "Train Epoch: 4 [25600/60000(43%)]\tLoss: 0.170463\n",
      "Train Epoch: 4 [32000/60000(53%)]\tLoss: 0.117574\n",
      "Train Epoch: 4 [38400/60000(64%)]\tLoss: 0.069504\n",
      "Train Epoch: 4 [44800/60000(75%)]\tLoss: 0.147245\n",
      "Train Epoch: 4 [51200/60000(85%)]\tLoss: 0.087121\n",
      "Train Epoch: 4 [57600/60000(96%)]\tLoss: 0.109560\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy:57648/60000 (96%)\n",
      "Train Epoch: 5 [0/60000(0%)]\tLoss: 0.065869\n",
      "Train Epoch: 5 [6400/60000(11%)]\tLoss: 0.184306\n",
      "Train Epoch: 5 [12800/60000(21%)]\tLoss: 0.054988\n",
      "Train Epoch: 5 [19200/60000(32%)]\tLoss: 0.073469\n",
      "Train Epoch: 5 [25600/60000(43%)]\tLoss: 0.242452\n",
      "Train Epoch: 5 [32000/60000(53%)]\tLoss: 0.146473\n",
      "Train Epoch: 5 [38400/60000(64%)]\tLoss: 0.063831\n",
      "Train Epoch: 5 [44800/60000(75%)]\tLoss: 0.221975\n",
      "Train Epoch: 5 [51200/60000(85%)]\tLoss: 0.030096\n",
      "Train Epoch: 5 [57600/60000(96%)]\tLoss: 0.072412\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy:57872/60000 (96%)\n",
      "\r"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    epochs = 5\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader, epoch)\n",
    "        save_model(model, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개선 방안 및 시도\n",
    "\n",
    "### 1. MLP 층 늘리기\n",
    "```\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)           # 층을 하나 더 추가해서 정확도가 증가하는지 측정\n",
    "```\n",
    "> - 기존에 fc2까지 작성 후 정확도: 55112/60000 (92%) <br>\n",
    "> - 변경 후 fc3까지 작성 후 정확도: 55352/60000 (92%)\n",
    ">> 결과: 약 200개 정도의 데이터를 더 맞춰지만, 정확도 퍼센트에 대한 큰 변화는 없었다\n",
    "\n",
    "\n",
    "### 2. 출력함수, 손실함수 변화\n",
    "- `return F.log_softmax(x, dim=1)`\n",
    "- `criterion = nn.NLLLoss()`\n",
    "\n",
    "> 기존 softmax, crossentropy를 사용한 정확도: 55352/60000 (92%)\n",
    "> 변경 후 log_softmax, NLLLoss를 사용한 정확도: 57872/60000 (96%)\n",
    ">> 결과: 약 2500개를 더 예측하며 정확도 퍼센트를 4% 더 올리는 유의미한 결과를 얻었다\n",
    "\n",
    "\n",
    "#### - LogSoftmax 사용\n",
    " LogSoftmax는 주어진 차원에서 각 클래스에 대한 확률의 로그를 계산한다. <br>\n",
    " 이 함수는 확률을 로그 공간에서 다루어 안정성을 높이고, 손실함수에 사용한 Negative Log Likelihood Loss를 위해 적용된다고 한다.\n",
    "\n",
    "#### - NLLLoss 사용\n",
    "CrossEntropyLoss는 내부적으로 LogSoftmax를 수행한다고 한다, 모델의 출력에 LogSoftmax를 사용하면서 NLLLoss를 사용해보았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
