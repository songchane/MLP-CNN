{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "- 사용 데이터: CIFAR10\n",
    "- 데이터 크기: (60000, 32, 32) \n",
    "\n",
    "이 튜토리얼은 CIFAR10 숫자를 분류하기 위해 합성곱 신경망(Convolutional Neural Network, CNN)을 훈련합니다. <br>\n",
    "이 모델을 CIFAR10 테스트 세트에서  71%정확도를 달성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 임포트하기\n",
    "- torch 라이브러리 install을 위해 참고할 사이트: https://pytorch.org/get-started/locally/\n",
    "- numpy 라이브러리 install을 위해 사용한 코드: `pip install numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로드\n",
    "- 데이터 불러오기(pytorch에 내장된 데이터, 처음 실행 시 지정 폴더에 다운됨)\n",
    "- 데이터 구성을, 학습/테스트 형태로 구분\n",
    "- MNIST 데이터와 같이 비전 모델 개발에 자주 사용되는 데이터는 학습, 테스트 데이터 구성 이미 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((16, 16)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 구현\n",
    "- 모델은 두 개의 합성곱 레이어 (conv1 및 conv2), 맥스 풀링 레이어 (pool), 그리고 두 개의 완전 연결 레이어 (fc1 및 fc2)로 구성됩니다.\n",
    "- 각 합성곱 레이어 이후에는 ReLU 활성화 함수가 적용됩니다.\n",
    "- 출력은 분류를 위해 로그 소프트맥스 활성화 함수를 통과합니다.\n",
    "\n",
    "### 모델 객체 생성\n",
    "- 모델을 훈련 모드로 설정하고 (model.train()), 배치를 반복하며 손실을 계산하고 역전파를 통해 모델 파라미터를 업데이트합니다.\n",
    "- 모델을 평가 모드로 설정하고 (model.eval()), 그래디언트를 업데이트하지 않고 손실과 정확도를 계산합니다.\n",
    "\n",
    "### 모델 학습 메소드 구현\n",
    "- device: GPU 사용 여부\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, 3, stride=1, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 43)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 256 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):   \n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   # 전체 데이터를 barch size만큼 읽어오는 과정 반복\n",
    "        data, target = data.to(device), target.to(device)       # GPU 사용시 CPU -> GPU\n",
    "        optimizer.zero_grad()       # Gradient 초기화, 매 학습시마다 초기화 후 backward\n",
    "        output = model(data)        # 데이터를 MLP에 통과치켜 output 획득\n",
    "        loss = criterion(output, target)        # output 벡터와 정답을 이용해 손실값 계산\n",
    "        loss.backward()             # 계산된 손실값으로 backward 진행(변화량 계산, 미분 계산)\n",
    "        optimizer.step()            # 파라미터 업데이트\n",
    "        if batch_idx % 100 == 0:    # 결과값 10번 반복마다 print\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}({100.*batch_idx/len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()        # 모델 평가 시, eval() 적용해 평가모드 진입(학습시 필요 연산 비활성화)\n",
    "    test_loss = 0       # 테스트 데이터에 대한 누적 손실값 계산을 위한 변수\n",
    "    correct = 0         # 테스트 데이터들 중, 정답을 맞춘 데이터의 수를 세기 위한 변수\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()       # 손실함수를 통한 손실값 계산\n",
    "            pred = output.argmax(dim=1, keepdim=True)           # 0~9중 가장 높은 확률에 해당하는 값\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()   # 예측치와 실제 닶이 같은지 비교\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)           # 전체 데이터 수로 누적값을 나눠, 평균 손실값 계산\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy:{correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n', end='\\r')\n",
    "    # 전체 데이터에 대해 정답을 맞춘 비율(정확도) 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장 및 불러오기 함수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch):\n",
    "    torch.save(model.state_dict(), 'models/mnist_mlp_model{}.pth'.format(epoch))\n",
    "\n",
    "def load_model(model, model_path):\n",
    "    model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습과 저장\n",
    "다음 코드 결과 .pth로 저장 / models 파일 생성 해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000(0%)]\tLoss: 3.769187\n",
      "Train Epoch: 1 [12800/50000(26%)]\tLoss: 1.399992\n",
      "Train Epoch: 1 [25600/50000(51%)]\tLoss: 1.357658\n",
      "Train Epoch: 1 [38400/50000(77%)]\tLoss: 1.274638\n",
      "\n",
      "Test set: Average loss: 0.0090, Accuracy:5959/10000 (60%)\n",
      "Train Epoch: 2 [0/50000(0%)]\tLoss: 1.050650\n",
      "Train Epoch: 2 [12800/50000(26%)]\tLoss: 1.009012\n",
      "Train Epoch: 2 [25600/50000(51%)]\tLoss: 1.018012\n",
      "Train Epoch: 2 [38400/50000(77%)]\tLoss: 1.032301\n",
      "\n",
      "Test set: Average loss: 0.0078, Accuracy:6504/10000 (65%)\n",
      "Train Epoch: 3 [0/50000(0%)]\tLoss: 0.883776\n",
      "Train Epoch: 3 [12800/50000(26%)]\tLoss: 1.004939\n",
      "Train Epoch: 3 [25600/50000(51%)]\tLoss: 0.757790\n",
      "Train Epoch: 3 [38400/50000(77%)]\tLoss: 0.927450\n",
      "\n",
      "Test set: Average loss: 0.0072, Accuracy:6798/10000 (68%)\n",
      "Train Epoch: 4 [0/50000(0%)]\tLoss: 0.894832\n",
      "Train Epoch: 4 [12800/50000(26%)]\tLoss: 0.936428\n",
      "Train Epoch: 4 [25600/50000(51%)]\tLoss: 0.743931\n",
      "Train Epoch: 4 [38400/50000(77%)]\tLoss: 0.776116\n",
      "\n",
      "Test set: Average loss: 0.0069, Accuracy:6958/10000 (70%)\n",
      "Train Epoch: 5 [0/50000(0%)]\tLoss: 0.553889\n",
      "Train Epoch: 5 [12800/50000(26%)]\tLoss: 0.575478\n",
      "Train Epoch: 5 [25600/50000(51%)]\tLoss: 0.660617\n",
      "Train Epoch: 5 [38400/50000(77%)]\tLoss: 0.526218\n",
      "\n",
      "Test set: Average loss: 0.0067, Accuracy:7098/10000 (71%)\n",
      "\r"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    epochs = 5   \n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        save_model(model, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개선 방안 및 시도\n",
    "### 1. 파라미터 및 함수 변경\n",
    "- 최적화 함수: Adam(model.parameters(), lr=0.001)\n",
    "- 손실함수: CrossEntropyLoss()\n",
    "- batch_size: 128\n",
    "\n",
    "> - 함수 및 파라미터 변경 전 정확도: 5282/10000 (53%)\n",
    "> - 함수 및 파라미터 변경 후 정화도: 7098/10000 (71%)\n",
    ">> 결과: 기존 변경 전과 비교하여 약 2000개의 데이터를 잘 맞췄고 약 18%의 정확도가 증가하는 유의미한 결과를 확인할 수 있었다. <br> 이 결과를 통해 파라미터, 함수가 모델 생성 과정과 결과적으로 미치는 영향에 대해 체감하게 되었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
